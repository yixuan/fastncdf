---
title: "Fast Normal CDF"
output: 
  github_document:
    pandoc_args: --webtex=https://latex.codecogs.com/svg.latex?
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.path = file.path("man", "README_files", ""))
```

### Introduction

The standard normal CDF $\Phi(x)$ is an important function in a broad range of
statistical problems. When we need to evaluate the function many times
(for example in numerical integration), the computation performance may become
an issue.

One way to fast evaluate the function is to use a look-up table, that is, 
we pre-compute a set of pairs $(x_i, \Phi(x_i))$` and then use interpolation
to approximate the function value of a given $x$.

This simple library calculates the $\Phi(x)$ function using piecewise linear
interpolation. The approximation error is guaranteed to be no greater than
$\epsilon = 10^{-7}$.

### Installation 

The package can be installed from Github by calling:

```{r how_to_install, eval = FALSE}
remotes::install_github("boennecd/fastncdf")
```


### Algorithm

We need to first determine the knots $x_i$ that we want to pre-compute.
Since $\Phi(-x) = 1 - \Phi(x)$, we only need to consider non-negative 
$x_i$'s.

For $x > \Phi^{-1}(1 - \epsilon) = `r qnorm(1 - 1e-7)`$, 
we set $\Phi(x) = 1$ and hence the error is
bounded by $\epsilon$. Let $x_0 = 0$, $x_i = ih$, $i = 0, 1, ..., N$, where
$N$ is the smallest integer such that $N h > `r qnorm(1 - 1e-7)`$.
Then we need to determine the interval width $h$ to satisfy the error bound.

For piecewise linear interpolation, the error is bounded by

$$E(t) \leq 1/8 \cdot \lVert f''\rVert_{\infty}h^2$$

(Source [http://pages.cs.wisc.edu/~amos/412/lecture-notes/lecture09.pdf](http://pages.cs.wisc.edu/~amos/412/lecture-notes/lecture09.pdf))


Since $\Phi''(x) = \phi'(x) = -x \phi(x)$, it can be shown that 
$\lVert \Phi''\rVert_\infty = \psi(1) = `r dnorm(1)`$.

Therefore, $h$ can be calculated as:

```{r show_h}
(h <- sqrt(8 / dnorm(1) * 1e-7))
```

So the $x_i$s and $y_i$s values are:

```{r get_vals}
x <- seq(0, qnorm(1 - 1e-7) + h, by = h)
length(x)
y <- pnorm(x)
```

We can then call `dput(x)` and `dput(y)` to get the data we need. 

### Performance

We compare the speed of `fastpnorm()` and `fastpnorm_preallocated()` 
with `pnorm()` in R:

```{r comp_speed}
library(fastncdf)
u <- seq(-6, 6, by = 1e-6)
system.time(truth <- pnorm(u))
system.time(fasty <- fastpnorm(u))
system.time(fasty_prec <- fastpnorm(u, TRUE))

range(truth - fasty)
range(truth - fasty_prec)

# if we already had a vector with values then we can use a faster version
res <- rep(0., length(u))
system.time(fastpnorm_preallocated(u, res))
all.equal(res, fasty)
```

We plot the error versus the quantile below:

```{r err_plt, fig.height = 4, fig.width=7}
par(mar = c(5, 5, 1, 1))
us <- seq(-9, 9, length.out = 2000)
plot(us, fastpnorm(us) - pnorm(us), type = "h",
     bty = "l", xlab = expression(x), ylab = "Error")
abline(h = 0, lty = 2)

plot(us, fastpnorm(us, TRUE) - pnorm(us), type = "h",
     bty = "l", xlab = expression(x), ylab = "Error")
abline(h = 0, lty = 2)
```

### Other Interpolation Methods

We can get a similar result using R's `approxfun` to do linear 
interpolation:

```{r R_lin_aprx}
lin_aprx <- local({
  f <- approxfun(x = x, y = y, yleft = 0.5, yright = 1)
  function(x){
    p <- f(abs(x))
    ifelse(x < 0, 1 - p, p)
  }
})

max(abs(lin_aprx(u) - fastpnorm(u)))
```

The R version is slower though:

```{r time_R_lin_aprx}
system.time(lin_aprx(u))
system.time(fastpnorm(u))
```

We can though use R's `splinefun` to see the performance of other functions. 
In particular, we can consider monotone cubic interpolation using 
Fritschâ€“Carlson method:

```{r monotone_spline}
m_splin <- local({
  n_points <- 300L
  eps <- 1e-9
  x <- seq(0, qnorm(1 - eps), length.out = n_points)
  f <- splinefun(x = x, y = pnorm(x), method = "monoH.FC")
  x_max <- max(x)
  
  function(x){
    p <- f(abs(x))
    out <- ifelse(x < 0, 1 - p, p)
    ifelse(abs(x) > x_max, .5 * (1 + sign(x)), out)
  }
})

# check the error 
range(truth - m_splin(u))

# plot the error
plot(us, m_splin(us) - pnorm(us), type = "h",
     bty = "l", xlab = expression(x), ylab = "Error")
```

We will require three doubles per knot unlike the two we need for 
the linear interpolation. Furthermore, more computation is needed to 
perform the interpolation. However, we may need much fewer knots as shown 
above and this will reduce the cache misses.

A C++ implementation is also provided with this package:

```{r cpp_version_of_mcubic}
all.equal(m_splin(u), fastpnorm(u, use_cubic = TRUE))

system.time(aprx_cubic <- fastpnorm(u, use_cubic = TRUE))
max(abs(aprx_cubic - truth))

res <- rep(0., length(u))
system.time(fastpnorm_preallocated(u, res, use_cubic = TRUE))
all.equal(res, aprx_cubic)
```

We can compare the monotone cubic interpolation with the linear 
interpolation with a particular focus on how well they scale in the number 
of threads used in the computation:

```{r bench}
res <- rep(0, length(u))
test_func <- function(use_cubic, n_threads)
  fastpnorm_preallocated(u, res, n_threads = n_threads, 
                         use_cubic = use_cubic)

bench::mark(
  `pnorm             ` = pnorm(u),
  `linear (1 thread) ` = test_func(FALSE, 1L),
  `linear (2 threads)` = test_func(FALSE, 2L),
  `linear (4 threads)` = test_func(FALSE, 4L),
  `linear (6 threads)` = test_func(FALSE, 6L),
  `cubic  (1 thread) ` = test_func(TRUE , 1L),
  `cubic  (2 threads)` = test_func(TRUE , 2L),
  `cubic  (4 threads)` = test_func(TRUE , 4L),
  `cubic  (6 threads)` = test_func(TRUE , 6L),
  check = FALSE, min_time = 2)
```

We may prefer the monotone cubic 
interpolation given the lower error, lower memory 
requirements, it scales better in the number of threads, 
and it has less "sided" errors.

```{r get_info, eval = FALSE, echo = FALSE}
ev <- environment(environment(m_splin)$f)
dput(ev$dx[1]) # h

dput(ev$m)
length(ev$m)

dput(ev$x0)
length(ev$x0)

dput(ev$y0)
length(ev$y0)

dput(ev$x0)
dput(c(mapply(c, ev$m, ev$y0)))
```
